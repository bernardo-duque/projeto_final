---
title: "**Trabalho Final**"
subtitle: "Estatística e Ciência de Dados - PUC-Rio Verão 2022"
author: "Bernardo Duque"
date: "27/02/2022"
output:
  pdf_document:
    toc: true
    number_sections: true
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---
```{r setup, echo = F, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,echo = F) 
setwd("C://Users//duque//Desktop//PUC//Mestrado//Verão 1//Estatística//Data Science//Trabalho Final//output")
library(dplyr)
library(kableExtra)
library(ggplot2)
library(wordcloud)
load("estatisticas_gerais.rda")
load("estatisticas_senadores.rda")
```

 
# Objetivo

<font size="4"> 

\par    Este trabalho teve como objetivo analisar os discursos e a composição da Comissão Parlamentar de Inquérito (CPI) da Pandemia, que teve início em 27 de abril de 2021 e fim em 26 de outubro do mesmo ano. A comissão teve ampla divulgação midiática e alto acompanhamento por parte da população^[De acordo com dados do Instituto DataSenado, 73% da população tomou conhecimento da CPI. Fonte: https://www12.senado.leg.br/noticias/materias/2021/07/19/datasenado-73-dos-brasileiros-conhecem-a-cpi-da-pandemia]. Sua relevância se deu principalmente pelo descaso do Governo Federal no combate à crise sanitária instalada no país e no mundo.

\par    Para isso, foi criado um modelo de *Machine Learning (ML)* para automatizar a análise, indicando se o discurso era favorável ou contra atuação da União durante a pandemia. Também foram construídas estatísticas descritivas e mapas com a composição dos participantes da CPI, com o output do modelo de ML e com o cruzamento dos dados de covid no país.

\par Vale destacar que este trabalho não tem a pretensão de ser conclusivo e apresenta diversas limitações que serão tratadas na última [seção](#limitacoes). O modelo não performou como esperado, portanto a ideia aqui é mostrar que a análise é possível, mas melhorias são necessárias; ou seja, servir como uma *prova de conceito*.

</font> 

# Dados Utilizados

\par    Para realizar a análise, foram utilizadas inicialmente 3 bases de dados, a saber:

* **[Base com discursos da CPI da Pandemia](https://basedosdados.org/dataset/br-senado-cpipandemia)**

  * Principal base utilizada neste trabalho. Contém todas as falas, tanto de senadores quanto de depoentes, no decorrer da CPI, incluindo tambem falas de ordem e de reuniões de requerimentos

* **[Dados de covid](https://brasil.io/dataset/covid19/files/)**

  * Base com dados acumulados da pandemia, com acumulação diária. Inclui número de casos, número de mortes, taxas por cem mil habitantes e a unidade de análise está no nível dos estados 

* **[Dados dos candidatos](https://basedosdados.org/dataset/br-tse-eleicoes)**
  
  * Base com informações dos candidatos, como idade, instrução, profissão, gênero, entre outras

\par As bases podem ser acessadas diretamente pelos link acima ou clicando [aqui.](https://drive.google.com/drive/folders/13ozE9GhQaxSmfw-wjO_2QT_frnw5nHxU?usp=sharing)

# Metodologia

## Tratamento das strings

\par Como visto na seção (#Dados Utilizados), a principal base utilizada foi a relativa aos discursos da CPI da Pandemia. Por se tratar de uma base cujos dados principais estavam no formato de texto (string), foi necessário um tratamento considerável dos dados para convertê-los em informação. Foram analisadas apenas as falas dos senadores; as dos depoentes foram filtradas. 

\par De forma geral, o processo adotado para tratar as strings foi a chamada tokenização. Pegou-se todas as palavras contidas em cada discurso e cada uma foi transformada em uma linha com as demais informações duplicadas do discurso. Em seguida, foram retiradas as chamadas stop words, que são palavras que não acrescentam informação, mas geram ruído nas análises e aumentam o custo de processamento do algoritmo. 

\par Desses primeiros passos, já foi possível extrair alguma informação, por meio de nuvens de palavras, que indicam as palavras que tiveram maior frequência durante os discursos. Uma foi gerada após esses passos já explicados, enquanto a segunda foi feita após o processo de *stemming*. Isto é, foram agrupadas palavras com bases em seus radicais; isso permite reduzir o número de palavras analisadas, focando em seus significados, ou seja, reduzir a quantidade de variáveis sem perder informação.

\par Em seguida, foram retiradas as palavras com baixa frequência, que apareciam menos de 20 vezes e correspodiam a 79% de todas as palavras. Por fim, cada palavra "stemizada" foi transformada em uma coluna (variável), permitindo a aplicação do modelo de ML.

## Machine Learning

\par  Anterior a isso, porém, foi gerada uma amostra aleatória replicável de 1000 observações da base original com os discursos^[Foi utilizada a função set.seed()]. As falas foram classificadas como:

* **Contra a atuação do Governo Federal na pandemia**

  * Inclui menções criticando a atuação do governo de forma geral, por meio da crítica a seus ministérios ou à própria pessoa do Presidente da República
  
  * Exemplo: INSERIR IMAGEM
  
* **A favor da atuação do Governo Federal na pandemia**

  * Inclui menções que louvam a atuação do governo de forma geral, por meio de apologia a medicamentos do chamado Kit Covid e defesas às críticas de outros senadores
  
  * Exemplo: INSERIR IMAGEM
  
* **Neutras ou de questões de ordem**

  * Inclui falas que não criticam nem defendem o Governo Federal, falas sobre requerimentos, perguntas sem juízo de valor ou questões de ordem corriqueiras de uma CPI
  
  * Exemplo: INSERIR IMAGEM
  
\par Com a classificação manual da amostra e com o tratamento das strings, finalmente foi possível aplicar um modelo de ML. A partir da classificação manual, foram separadas uma base de treino e outra de teste para o modelo, na proporção 65/35 respectivamente. Ou seja, 650 falas foram utilizadas como treino do modelo e 350 como teste. Importante ressaltar que elas foram alocadas aleatoriamente para cada um desses grupos. 

\par Foram testados 2 tipos de modelos de classificação: 

* **Random Forests**

  * EXPLICAR INTUIÇÃO DO MODELO
  
* **Extreme Gradient Boosting Tree (XGBTree)**

  * EXPLICAR INTUIÇÃO DO MODELO
  
\par Para ambos foi utilizada uma técnica chamada de cross-validation, que serve para evitar um *overfitting* do modelo. Intuitivamente, consiste em particionar sua base de treino e rotacionar iterativamente as partições como treino e teste, e testar a performance do modelo nessas subamostras

## Métricas de Validação

\par Após rodar os dois modelos, foram geradas as matrizes de confusão de cada um, e com base na métrica F1, foi o escolhido o modelo XGBTree, que estava melhor especificado. Infelizmente, o modelo não performou tão bem quanto o esperado, mas algumas tentativas podem ser feitas para melhorá-lo, como será discutido na seção [Limitações](#limitacoes).

\par A matriz de confusão apresenta as seguintes métricas: 

* **Precision**: Indica a proporção de positivos verdadeiros classificados como positivos dentre o universo das observações que foram classificadas como positivas. Precision  baixa aponta classificação errada dos negativos verdadeiros. É igual a 1 quando todas as classificações de positivos estão corretas.

  * Calculada por: $$ Precision = \frac{Positivos\ Verdadeiros}{Positivos\ Verdadeiros + Falsos\ Positivos} $$

* **Recall**: Mede a proporção de positivos verdadeiros classificados como positivos em relação ao universo de todos os positivos verdadeiros. Recall é igual a 1 quando todos os positivos verdadeiros foram classificados como positivos. Valor baixo indica classificação errada de positivos verdadeiros.

  * Calculada por: $$ Recall = \frac{Positivos\ Verdadeiros}{Positivos\ Verdadeiros + Falsos\ Negativos} $$

* **F1-Score**: Adequada quando busca-se um equilíbrio entre *Precision* e *Recall* - indiferença entre falsos positivos e falsos negativos - e quando um há uma distribuição heterogênea tendendo para um grande número de negativos verdadeiros. Na realidade é a média harmônica das duas medidas.

  * Calculada por : $$ F1 = 2*\frac{Precision * Recall}{Precision + Recall} $$
  
* **Prevalence**: Proporção de vezes que determinada classe apareceu na classificação manual

\par A tabela 1 apresenta a matriz de confusão do modelo utilizado: 

```{r, echo=F}
load("matriz.rda")
matriz %>% 
  kable(caption = "Matriz de Confusão do Modelo") %>%
  kable_classic(html_font = "Cambria",font_size=10) %>%
  column_spec(1, bold = T) %>%
  row_spec(0,bold=T) %>%
  kable_styling(latex_options = c("hold_position"))
```


\par Como dito anteriormente, as métricas deixam a deixar para as classificações "a favor do governo" e "contra o governo". Para a primeira, tanto o *precision* quanto o *recall* estão bem baixos, o que implica que o modelo está pegando poucos casos que deveria pegar, e os que está pegando está pegando errado. Na segunda classe, o *recall* também está bem baixo, implicando que o modelo está pegando menos casos do que deveria, mas o *precision* está num patamar mais aceitável, dando mais confiança para os discursos que estão classificados como tal.

\par Assim, as estatísticas encontradas devem ser analisadas com muita cautela e não devem ser tomadas como verdade, mas como um exercício para mostrar que a análise aqui empreendida é viável, desde que possua melhorias. 

# Estatísticas Descritivas

## Nuvens de palavras

\par A Figura 4 apresenta a primeira wordcloud, que foi construída após a tokenização dos discursos, enquanto a Figura 5 apresenta a wordcloud construída após o processo de *stemming*.

![Nuvem de Palavras - antes do *stemming*](wordcloud_cpi.png)

![Nuvem de Palavras - após *stemming*](wordcloud_cpi_stem.png)

\par Fica evidente das duas figuras que as palavras mais ditas durante a comissão foram "Presidente", "Senador" e "Senhor". Também aparecem algumas abreviações como "Sr"(senhor) e "V"(Vossa Excelência). 

\par Após o stemming, as palavras "vacinas","vacina" e possivelmente outras variações foram agrupadas sob "vacin" e ganharam maior destaque na nuvem de palavras. Entretanto, a palavra "senhor" e "senhora" nao foram agrupados sob o mesmo radical, como se esperaria. Esse problema se dá devido à função utilizada de stemming, que infelizmente têm menos opções em português do que em inglês. O mesmo problema ocorre vale para stopwords, com a presença de palavras como "até" e "lá".

\par Outras palavras que merecem destaque são "ministério", "saude", "ministro" e "governo", que como era de se esperar numa CPI de saúde pública aparecem com bastante frequência. 

## Dados obtidos do ML

### Discursos Totais

\par A Figura 6 apresenta o número total de discursos por senadores durante a CPI. 

### Discursos por Senadores 

\par A Figura

```{r, fig.width=8,fig.height=6}

a <- estatisticas_senadores %>%
  arrange(desc(total))

a <- a[1:5,]
a$nome_discursante <- c("O.Aziz","R.Calheiros","Randolfe","M.Rogerio","Elizane")

a %>%
  ggplot(aes(x = nome_discursante,y = total),
         alpha = 0.9, col="white") +
  geom_bar(stat='identity',fill = "darkblue") +
  labs(title = "Total de Discursos por Senador",subtitle = "Top 5 senadores",x="",y="") +
  theme(plot.title = element_text(face = "bold"), legend.title = element_blank()) + 
  geom_text(aes(label = total), vjust = 2, size = 5, color = "#ffffff") + 
  theme_bw()

```


```{r}

a <- estatisticas_senadores %>%
  arrange(desc(favor))

a <- a[1:5,]

a %>%
  ggplot(aes(x = nome_discursante,y = favor),
         alpha = 0.9, col="white") +
  geom_bar(stat='identity',fill = "#339900") +
  labs(title = "Total de Discursos a Favor do Governo por Senador",subtitle = "Top 5 senadores",x="",y="") +
  theme(plot.title = element_text(face = "bold"), legend.title = element_blank()) + 
  geom_text(aes(label = favor), vjust = 2, size = 5, color = "#ffffff") +
  theme_bw()

```



# Conclusão e Limitações {#limitacoes}

\par Infelizmente, por uma restrição temporal, não foi possível apresentar o trabalho e realizar todas as análises pensadas inicialmente. Algumas melhorias poderiam ser bem simples, mas acabaram não sendo priorizadas, como por exemplo a análise dos dados dos senadores.

\par Alguns nomes de senadores acabaram se perdendo. Seria necessário utilizar Regex para filtrar corretamente nomes como dos senadores Randolfe Rodrigues, Marcos Rogerio, Fabiano Contarato, Styvenson Valentim ou então utilizar um dicionário manual de nomes para dar o match. Assim, a análise por gênero, raça e partido acabou ficando limitada e não representativa de toda a amostra. 
  
\par Outro fator que poderia ter sido melhorado em relação ao modelo de ML é que a imensa maioria das falas não passavam informação alguma em relação ao que se estava tentando medir.Há também muitas interrupcoes nas falas, e falas que nao são discursos de fato, mas palavras de ordem. Na base existiam colunas que indicavam questões de ordem e falas fora do microfone, por exemplo, mas elas acabavam filtrando muitos casos que gostaríamos de medir, e deixando passar muitos dos quais nós não gostaríamos. Portanto, optou-se por não utilizá-las como filtro. 

\par  Porém, uma medida que poderia ser tomada era tentar tratar melhor esses dados de forma a filtrar essas falas que não nos transmitem informações. Também poderia-se pensar em maneiras de juntar as partes dos discursos interrompidos transformando-os em uma fala coesa e contínua.

\par Outra forma de também aprimorar o modelo seria aumentando o número de observações da base de treino. Se tivéssemos um n=2000 ou n=3000, provavelmente teríamos um *fit* melhor, já que haveriam mais exemplos das classes para a máquina aprender os padrões corretamente. 

\par Por fim, com o modelo bem especificado, poderíamos então abordar a motivação inicial deste projeto, que seria cruzar os discursos com os dados de covid, e ver qual a relação entre número de casos/mortes por estado com o tipo de discurso dos senadores. Com isso, conseguiríamos ver, por exemplo, se senadores mais críticos na CPI à atuação do governo na pandemia vêm de estados que sofreram mais com a crise sanitária.   
